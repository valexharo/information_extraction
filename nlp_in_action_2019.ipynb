{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_in_action_2019.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valexharo/information_extraction/blob/master/nlp_in_action_2019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fvCVJr6v4n38"
      },
      "source": [
        "# Natural Language Processing in action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kb4EaWev4n39"
      },
      "source": [
        "Este _notebook_ ilustra la facilidad de uso y la flexibilidad de [__spaCy__](https://spacy.io/) `>= 2` para procesar material textual. Este _notebook_ fue utilizado durante nuestra comunicación en WOMEN IN DATA SCIENCE VALENCIA 2020 que tuvo lugar en Valencia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ITGXUbHe4n3-"
      },
      "source": [
        "## Objetivo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7H7-3d_i4n3_"
      },
      "source": [
        "Con este _notebook_ aprenderás:\n",
        "\n",
        "1. los conceptos básicos de __spaCy__\n",
        "1. cómo encontrar en un texto las menciones de ciertos términos\n",
        "1. cómo extraer información estructurada de un texto usando la anotación lingüística que aporta __spaCy__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g0_etDYP4n3_"
      },
      "source": [
        "## Preliminares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IuZn68OD4n4A"
      },
      "source": [
        "Necesitarás tener instalado en tu entorno las siguientes dependencias:\n",
        "\n",
        "- __spaCy__ \n",
        "- el modelo de lenguaje `en_core_web_md`\n",
        "\n",
        "Un modelo de lenguaje es una clase cuyo corazón lo constituye una serie de reglas y un modelo estadístico y un pipeline para llevar a cabo:\n",
        "\n",
        "- tokenización\n",
        "- lematización\n",
        "- etiquetado de palabras por categoría gramatical\n",
        "- anotación sintáctica de dependencias\n",
        "- anotación de entidades nominadas\n",
        "- identificación de oraciones\n",
        "\n",
        "El modelo es una red neuronal convolucional (CNN) entrenada sobre el corpus OntoNotes y viene con vectores Glove entrenados sobre el corpus Common Crawl. Estos vectores permiten realizar operaciones semánticas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bgvn2zIo4n4B"
      },
      "source": [
        "Puedes instalarlos ejecutando los siguientes comandos:\n",
        "\n",
        "```shell\n",
        "pip install -U spacy\n",
        "python -m spacy download en_core_web_md\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O-uY049g4n4B"
      },
      "source": [
        "## Lo básico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FILnfxa_4n4C"
      },
      "source": [
        "Importa los módulos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E7sq1ze14n4C",
        "colab": {}
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SVkRZB2z4n4F"
      },
      "source": [
        "[`Doc`](https://spacy.io/api/doc) es un objeto que representa un texto analizado con __spaCy__ que, a su vez, contiene objetos tipo `Token` y `Span`. Un [`Token`](https://spacy.io/api/token) es la estructura de datos para representar una palabra y su anotación lingüística. Un [`Span`](https://spacy.io/api/span) es una sección de un `Doc`. Por ejemplo, una frase es un objeto `Span`, una entidad nominada (Named Entity) es también un objeto `Span`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GY2hgKUg4n4F",
        "colab": {}
      },
      "source": [
        "from spacy.tokens import Doc, Span"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yT7y-hB54n4H"
      },
      "source": [
        "El [`Matcher`](https://spacy.io/api/matcher) es una clase que permite encontrar secuencias de tokens basada en reglas. Dichas [reglas](https://spacy.io/usage/linguistic-features#section-rule-based-matching) nos recuerdan a las expresiones regulares pero aprovechando toda la información lingüística que añade __spaCy__ al parsear un texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y-91p2gd4n4I",
        "colab": {}
      },
      "source": [
        "from spacy.matcher import Matcher"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SkWDhjX14n4J"
      },
      "source": [
        "__spaCy__ viene con un visualizador para mostrar la anotación lingüística, se llama [__displaCy__](https://spacy.io/usage/visualizers). Aquí puedes encontrar una [demo](https://explosion.ai/demos/displacy-ent)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "al1qJWFI4n4K",
        "colab": {}
      },
      "source": [
        "from spacy import displacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l-zqtMr_4n4L",
        "colab": {}
      },
      "source": [
        "from spacy.symbols import NOUN, PROPN, VERB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "09DAqfo04n4N",
        "colab": {}
      },
      "source": [
        "from itertools import takewhile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa4wmtcNEY8j",
        "colab_type": "text"
      },
      "source": [
        "Vamos a instalar el modelo de spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM20o2cGDuvF",
        "colab_type": "code",
        "outputId": "cbd9d391-0ca6-4fbd-ca7a-101c883bacd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IaANidTN4n4P"
      },
      "source": [
        "Ahora, carga el modelo..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wE00nm8y4n4P"
      },
      "source": [
        "tarda unos segundos..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4yb45dXg4n4Q",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F_bbENSw4n4S"
      },
      "source": [
        "Pipes `==` etapas, spaCy nos permite organizar `pipelines` o flujos de procesamiento de texto. Para una expliación detallada del paradigma y lo que hace cada una de las etapas que vienen con los modelos de __spaCy__ consulta la [documentación](https://spacy.io/usage/processing-pipelines)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WXGPRLYV4n4S",
        "outputId": "539c4225-b549-44ab-e91d-8c14f08a2105",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "nlp.pipe_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tagger', 'parser', 'ner']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YJyrKHH84n4V"
      },
      "source": [
        "- [_tagger_](https://spacy.io/usage/linguistic-features#section-pos-tagging) es el anotador de la categoría gramatical de las palabras (sustantivo, adjetivo, verbo, preposición...)\n",
        "- [_parser_](https://spacy.io/usage/linguistic-features#section-dependency-parse) es el anotador sintáctica que anota la función de cada palabra dentro de la frase (sujeto, verbo, objeto directo...)\n",
        "- [_ner_](https://spacy.io/usage/linguistic-features#section-named-entities) es el modulo para identificar entidades nominadas (nombres de personas, países, ciudades, organizaciones, cantidades, fechas...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0-9CHs7e4n4V"
      },
      "source": [
        "Ahora vamos a analizar el siguiente fragmento de texto con __spaCy__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ifT6YirH4n4W",
        "colab": {}
      },
      "source": [
        "text = '''Uber pays $148 mn over data breach in latest image-boosting move. \n",
        "SAN FRANCISCO.\n",
        "Uber agreed Wednesday to pay a $148 million penalty over a massive\n",
        "2016 data breach which the company concealed for a year, in the latest effort by the \n",
        "global ridesharing giant to improve its image and move past its missteps from its early years.'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1EHfh5wI4n4X"
      },
      "source": [
        "Analiza lingüísticamente el texto con el modelo y devuelve un objeto de la clase `Doc`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vkd8n9zA4n4Y",
        "colab": {}
      },
      "source": [
        "doc = nlp(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VkQhWCJx4n4a"
      },
      "source": [
        "El `Doc` contiene un generador de oraciones (`sents`) y todas las palabras (`Token`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lSrmtW1j4n4a",
        "outputId": "cea8c152-141d-47c3-f3ed-459157f9b428",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for sent in doc.sents:\n",
        "    for token in sent:\n",
        "        if not token.is_space:\n",
        "            print(\"{:<15}{:<15}{}\".format(\n",
        "                token.text,  # la palabra tal y como apareció en el texto\n",
        "                token.lemma_,  # su forma lematizada\n",
        "                token.pos_  # la categoría gramatical de la palabra\n",
        "            ))\n",
        "    print('\\n')  # cada línea en blanco marca el final de una frase"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uber           Uber           PROPN\n",
            "pays           pay            VERB\n",
            "$              $              SYM\n",
            "148            148            NUM\n",
            "mn             mn             NOUN\n",
            "over           over           ADP\n",
            "data           datum          NOUN\n",
            "breach         breach         NOUN\n",
            "in             in             ADP\n",
            "latest         late           ADJ\n",
            "image          image          NOUN\n",
            "-              -              PUNCT\n",
            "boosting       boost          VERB\n",
            "move           move           NOUN\n",
            ".              .              PUNCT\n",
            "\n",
            "\n",
            "SAN            SAN            PROPN\n",
            "FRANCISCO      FRANCISCO      PROPN\n",
            ".              .              PUNCT\n",
            "\n",
            "\n",
            "Uber           Uber           PROPN\n",
            "agreed         agree          VERB\n",
            "Wednesday      Wednesday      PROPN\n",
            "to             to             PART\n",
            "pay            pay            VERB\n",
            "a              a              DET\n",
            "$              $              SYM\n",
            "148            148            NUM\n",
            "million        million        NUM\n",
            "penalty        penalty        NOUN\n",
            "over           over           ADP\n",
            "a              a              DET\n",
            "massive        massive        ADJ\n",
            "2016           2016           NUM\n",
            "data           datum          NOUN\n",
            "breach         breach         NOUN\n",
            "which          which          DET\n",
            "the            the            DET\n",
            "company        company        NOUN\n",
            "concealed      conceal        VERB\n",
            "for            for            ADP\n",
            "a              a              DET\n",
            "year           year           NOUN\n",
            ",              ,              PUNCT\n",
            "in             in             ADP\n",
            "the            the            DET\n",
            "latest         late           ADJ\n",
            "effort         effort         NOUN\n",
            "by             by             ADP\n",
            "the            the            DET\n",
            "global         global         ADJ\n",
            "ridesharing    ridesharing    NOUN\n",
            "giant          giant          NOUN\n",
            "to             to             PART\n",
            "improve        improve        VERB\n",
            "its            -PRON-         DET\n",
            "image          image          NOUN\n",
            "and            and            CCONJ\n",
            "move           move           VERB\n",
            "past           past           ADP\n",
            "its            -PRON-         DET\n",
            "missteps       misstep        NOUN\n",
            "from           from           ADP\n",
            "its            -PRON-         DET\n",
            "early          early          ADJ\n",
            "years          year           NOUN\n",
            ".              .              PUNCT\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ySC2VuVd4n4c"
      },
      "source": [
        "También podemos recuperar las entidades nominadas que ha encontrado en el texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ywoKAO9t4n4d",
        "outputId": "c295a70a-1a17-4ba9-d3a1-ab696672753f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc.ents"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(148, SAN FRANCISCO, Uber, Wednesday, $148 million, 2016, a year, early years)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wfwTeoAJ4n4e",
        "outputId": "baad045b-c4a3-4df0-ba82-802595719661",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "for ent in doc.ents:\n",
        "    print(\"{:<15}{}\".format(\n",
        "        ent.text,  # el texto marcado como entidad nominada, pueden ser una o más palabras\n",
        "        ent.label_  # la categoría adjudicada por spaCy\n",
        "    ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "148            MONEY\n",
            "SAN FRANCISCO  GPE\n",
            "Uber           PERSON\n",
            "Wednesday      DATE\n",
            "$148 million   MONEY\n",
            "2016           DATE\n",
            "a year         DATE\n",
            "early years    DATE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hESfyBB44n4g"
      },
      "source": [
        "## Cómo encontrar términos en un texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BuQ1Amw_4n4h"
      },
      "source": [
        "Imagina que esta es nuestra ontología. Las claves del diccionario son los conceptos, y los elementos de la lista son los términos, que a su vez están compuestos de una lista de palabras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EMMKKPCz4n4h",
        "colab": {}
      },
      "source": [
        "terminology = {\n",
        "    \"CstSec\": [\n",
        "        [{'LOWER':'data'}, {'LOWER':'breach'}],\n",
        "        [{'LOWER':'data'}, {'LOWER':'protection'}],\n",
        "        [{'LOWER':'personal'}, {'LOWER':'information'}]\n",
        "    ]\n",
        "    ,\n",
        "    \"IntelPty\": [\n",
        "        [{'LOWER':'trade'}, {'LOWER':'secrets'}]\n",
        "    ]\n",
        "    ,\n",
        "    \"HuRgts\": [\n",
        "        [{'LOWER':'discrimination'}]\n",
        "    ]\n",
        "} "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C333-tRL4n4k"
      },
      "source": [
        "Recuerda las etapas que teníamos para procesar un texto en __spaCy__:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4vCRzPp64n4l",
        "outputId": "6cfb38d6-04fc-4644-aa9d-ff705687c47d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "nlp.pipe_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tagger', 'parser', 'ner']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V27OwO2p4n4o"
      },
      "source": [
        "A continuación vamos a definir una nueva etapa. Se trata de un objeto `Matcher` que nos permitará hacer la búsqueda de los términos en un objeto `Doc`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UV4-415j4n4o",
        "colab": {}
      },
      "source": [
        "class MyMatcher():\n",
        "    \n",
        "    def __init__(self, nlp, terminology, label='Match', function=None):  # el constructor de nuestro Matcher\n",
        "        \n",
        "        self.matcher = Matcher(nlp.vocab)  # creamos un objeto Matcher\n",
        "        for topic, patterns in terminology.items():  # cargamos los términos de la mini ontología\n",
        "            for term in patterns:\n",
        "                self.matcher.add(topic, function, term)\n",
        "        Doc.set_extension('rule_match', default=False, force=True)  # creamos una extensión al objeto Doc para poder \n",
        "                                                                    # almacenar esta información\n",
        "\n",
        "    def __call__(self, doc):  # la función que se llamará desde el pipeline de spaCy\n",
        "        matches = self.matcher(doc)  # aplicaremos el matcher sobre el Doc\n",
        "        \n",
        "        spans = []\n",
        "        for label, start, end in matches:  # para cada termino encontrado\n",
        "            span = Span(doc, start, end, label=label)  # crea un objeto Span\n",
        "            spans.append(span)\n",
        "            \n",
        "        \n",
        "        doc._.rule_match = spans  # guardamos los Spans en el atributo que habíamos declarado en el constructor\n",
        "        return doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JRdc5KFg4n4q"
      },
      "source": [
        "Declaramos el buscador de términos con nuestra terminología."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aNsIgsqR4n4r",
        "colab": {}
      },
      "source": [
        "my_matcher = MyMatcher(nlp, terminology, label='about')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f4J5Z0zS4n4s"
      },
      "source": [
        "Y lo añadimos a nuestra analizador lingüístico como una nueva etapa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DoxX1v4c4n4t",
        "colab": {}
      },
      "source": [
        "nlp.add_pipe(my_matcher, name=\"term_matcher\", after='ner')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xg_1SMJ34n4v"
      },
      "source": [
        "Ahora comprobamos que se ha añadido al pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_cPRTYmC4n4v",
        "outputId": "705b1427-e336-4a45-e3b6-5475bc109374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "nlp.pipe_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tagger', 'parser', 'ner', 'term_matcher']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PgQ6hUqU4n4x"
      },
      "source": [
        "## Cómo extraer información estructurado de un texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s7wl5_AG4n4y"
      },
      "source": [
        "Con la información lingúistica que nos proporciona __spaCy__ vamos a extraer quién (sujeto) hizo (verbo) qué (objeto) en aquellas frases donde se ha mencionado alguno de los términos de la ontología. Para ello usaremos la información sintáctica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2nnJy0dm4n4y"
      },
      "source": [
        "Este es un ejemplo de las depencias sintácticas anotadas por __spaCy__ para una frase muy simple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "04314jYa4n4z",
        "outputId": "67ed1cf3-c56a-48b0-a966-2a1a6f46454e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "displacy.render(nlp(\"Paul ate paella.\"), style='dep', jupyter=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"16a2dac3c2a84571928344afccd07735-0\" class=\"displacy\" width=\"575\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Paul</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">ate</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">paella.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-16a2dac3c2a84571928344afccd07735-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-16a2dac3c2a84571928344afccd07735-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-16a2dac3c2a84571928344afccd07735-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-16a2dac3c2a84571928344afccd07735-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M400.0,91.5 L408.0,79.5 392.0,79.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q046VBbI4n40"
      },
      "source": [
        "Ahora necesitaríamos escribir las reglas sintácticas para poder recuperar los elementos de la tupla que estamos buscando. En este caso vamos a tomar algunas funciones prestadas de [textacy](https://github.com/chartbeat-labs/textacy), que es un módulo muy interesante para realizar análisis textual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D7HMnSA54n41",
        "colab": {}
      },
      "source": [
        "SUBJ_list = ['agent', 'csubj', 'csubjpass', 'expl', 'nsubj', 'nsubjpass']\n",
        "OBJ_list = ['attr', 'dobj', 'dative', 'oprd', 'pobj']\n",
        "AUX_list = ['aux', 'auxpass', 'neg']\n",
        "prepositional_phrase = ['NOUN', 'ADP', 'PROPN']\n",
        "\n",
        "def get_main_verbs_of_sent(sent):\n",
        "    \"\"\"Return the main (non-auxiliary) verbs in a sentence.\"\"\"\n",
        "    return [tok for tok in sent\n",
        "            if tok.pos == VERB and tok.dep_ not in {'aux', 'auxpass'}]\n",
        "\n",
        "def get_subjects_of_verb(verb):\n",
        "    \"\"\"Return all subjects of a verb according to the dependency parse.\"\"\"\n",
        "    subjs = [tok for tok in verb.lefts\n",
        "             if tok.dep_ in SUBJ_list]\n",
        "    # get additional conjunct subjects\n",
        "    subjs.extend(tok for subj in subjs for tok in _get_conjuncts(subj))\n",
        "    return subjs\n",
        "\n",
        "def get_objects_of_verb(verb):\n",
        "    \"\"\"\n",
        "    Return all objects of a verb according to the dependency parse,\n",
        "    including open clausal complements.\n",
        "    \"\"\"\n",
        "    objs = [tok for tok in verb.rights\n",
        "            if tok.dep_ in OBJ_list]\n",
        "    # get open clausal complements (xcomp)\n",
        "    objs.extend(tok for tok in verb.rights\n",
        "                if tok.dep_ == 'xcomp')\n",
        "    # get additional conjunct objects\n",
        "    objs.extend(tok for obj in objs for tok in _get_conjuncts(obj))\n",
        "    return objs\n",
        "\n",
        "def get_preprositional_phrase(objs):\n",
        "    \"\"\"\n",
        "    Receive the object and check if the object have predical phrase\n",
        "    Here I want to extract patterns like: (NOUN + ADP + PROPN) or \n",
        "    (NOUN + ADP + NOUN)\n",
        "    \"\"\"\n",
        "    return [right for right in objs.rights\n",
        "            if right.dep_ == 'prep']\n",
        "\n",
        "def get_span_for_subtree(obj):\n",
        "    \n",
        "    min_i = obj.i\n",
        "    max_i = obj.i + sum(1 for _ in [right.subtree for right in obj.rights])\n",
        "    return (min_i, max_i)\n",
        "\n",
        "def get_span_for_compound_noun(noun):\n",
        "    \"\"\"\n",
        "    Return document indexes spanning all (adjacent) tokens\n",
        "    in a compound noun.\n",
        "    \"\"\"\n",
        "    min_i = noun.i - sum(1 for _ in takewhile(lambda x: x.dep_ == 'compound',\n",
        "                                              reversed(list(noun.lefts))))\n",
        "    return (min_i, noun.i)\n",
        "\n",
        "\n",
        "def get_span_for_verb_auxiliaries(verb):\n",
        "    \"\"\"\n",
        "    Return document indexes spanning all (adjacent) tokens\n",
        "    around a verb that are auxiliary verbs or negations.\n",
        "    \"\"\"\n",
        "    min_i = verb.i - sum(1 for _ in takewhile(lambda x: x.dep_ in AUX_list,\n",
        "                                              reversed(list(verb.lefts))))\n",
        "    max_i = verb.i + sum(1 for _ in takewhile(lambda x: x.dep_ in AUX_list,\n",
        "                                              verb.rights))\n",
        "    return (min_i, max_i)\n",
        "\n",
        "def _get_conjuncts(tok):\n",
        "    \"\"\"\n",
        "    Return conjunct dependents of the leftmost conjunct in a coordinated phrase,\n",
        "    e.g. \"Burton, [Dan], and [Josh] ...\".\n",
        "    \"\"\"\n",
        "    return [right for right in tok.rights\n",
        "            if right.dep_ == 'conj']\n",
        "\n",
        "\n",
        "def subject_verb_object(doc, start_i):\n",
        "    \"\"\"\n",
        "    Extract an ordered sequence of subject-verb-object (SVO) triples from a\n",
        "    spacy-parsed doc. Note that this only works for SVO languages.\n",
        "\n",
        "    Args:\n",
        "        doc (``textacy.Doc`` or ``spacy.Doc`` or ``spacy.Span``)\n",
        "\n",
        "    Yields:\n",
        "        Tuple[``spacy.Span``, ``spacy.Span``, ``spacy.Span``]: the next 3-tuple\n",
        "            of spans from ``doc`` representing a (subject, verb, object) triple,\n",
        "            in order of appearance\n",
        "    \"\"\"\n",
        "    \n",
        "    obj_pp =[]\n",
        "    verbs = get_main_verbs_of_sent(sent)\n",
        "    for verb in verbs:\n",
        "        subjs = get_subjects_of_verb(verb)\n",
        "        if not subjs:\n",
        "            continue\n",
        "        objs = get_objects_of_verb(verb)\n",
        "        if not objs:\n",
        "            continue\n",
        "                \n",
        "        for subj in subjs:\n",
        "            subj = sent[get_span_for_compound_noun(subj)[0] - start_i: subj.i - start_i + 1]\n",
        "            for obj in objs:\n",
        "                    \n",
        "                if obj.pos == NOUN:\n",
        "                    span = get_span_for_compound_noun(obj)\n",
        "                    obj_pp = get_preprositional_phrase(obj)\n",
        "                        \n",
        "                    if obj_pp:\n",
        "                        span = (span[0], get_span_for_subtree(obj_pp[0])[1])\n",
        "                            \n",
        "                elif obj.pos == VERB:\n",
        "                    span = get_span_for_verb_auxiliaries(obj)\n",
        "                    obj_pp = get_preprositional_phrase(obj)\n",
        "                    if obj_pp:\n",
        "                        span = (span[0], get_span_for_subtree(obj_pp[0])[1])\n",
        "                           \n",
        "                else:\n",
        "                    span = (obj.i, obj.i)\n",
        "                    obj_pp = get_preprositional_phrase(obj)\n",
        "                    if obj_pp:\n",
        "                        span = (span[0], get_span_for_subtree(obj_pp[0])[1])\n",
        "                obj = sent[span[0] - start_i: span[1] - start_i + 1]\n",
        "                yield (subj, verb, obj)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8EGiHmUF4n43"
      },
      "source": [
        "Ahora vamos a trabajar con un texto más largo, una noticia sobre Uber."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KZbo29tq4n44",
        "colab": {}
      },
      "source": [
        "text = '''Uber pays $148 mn over data breach in latest image-boosting move. \n",
        "SAN FRANCISCO - Uber agreed Wednesday to pay a $148 million penalty over a massive 2016 data breach which the company concealed for a year, in the latest effort by the global ridesharing giant to improve its image and move past its missteps from its early years.\n",
        "The settlement stems from a breach affecting some 57 million Uber riders and drivers, prompting litigation that was eventually joined by officials from the 50 US states and the District of Columbia.\n",
        "\n",
        "The payment, described as the largest in a data breach settlement, is part of Uber's efforts to burnish its reputation after a series of scandals over alleged misconduct and unethical practices.\n",
        "\n",
        "Uber disclosed the breach last year shortly after it hired chief executive Dara Khosrowshahi, who promised a new way of doing business as the company with an estimated value of more than $70 billion expands globally and prepares for what could be a massive stock offering.\n",
        "\n",
        "\"The commitments we're making in this agreement are in line with our focus on both physical and digital safety for our customers,\" Uber's chief legal officer Tony West said in announcing the settlement.\n",
        "\n",
        "\"We know that earning the trust of our customers and the regulators we work with globally is no easy feat ... We'll continue to invest in protections to keep our customers and their data safe and secure, and we're committed to maintaining a constructive and collaborative relationship with governments around the world.\"\n",
        "\n",
        "The company reached an agreement with the US Federal Trade Commission on the breach that called for improved security and audits but no financial penalty.\n",
        "\n",
        "According to officials, Uber paid data thieves $100,000 to destroy the swiped information -- and remained quiet about the breach for a year.\n",
        "\n",
        "The settlement avoid a potentially lengthy court fight which could be embarrassing to Uber.\n",
        "\n",
        "- Improving security -\n",
        "\n",
        "As part of the settlement, Uber will be required to improve its security practices, with an independent outside review of data practices.\n",
        "\n",
        "Illinois Attorney General Lisa Madigan said her office would oversee a fund of $5.1 million that would pay each driver from the state $100, and seek to locate those who may no longer be driving for Uber.\n",
        "\n",
        "\"While Uber is now taking the appropriate steps to protect the data of its drivers in Illinois and across the country, the company's initial response was unacceptable,\" Madigan said. \"Companies cannot hide when they break the law.\"\n",
        "\n",
        "New York Attorney General Barbara Underwood said: \"This record settlement should send a clear message: we have zero tolerance for those who skirt the law and leave consumer and employee information vulnerable to exploitation.\"\n",
        "\n",
        "The case is the second large court settlement this year for Uber.\n",
        "\n",
        "In February, Uber agreed to pay $245 million to Alphabet's self-driving car unit Waymo to settle a lawsuit over allegedly stolen trade secrets.\n",
        "\n",
        "As part of its transparency effort, Uber this year also scrapped policies requiring arbitration over claims of sexual misconduct involving employees, riders and drivers, allowing cases to be heard in public and pursued in open court.\n",
        "\n",
        "As a privately held firm, Uber is not required to report its finances. Released data from the second quarter however shows it lost $891 million on revenues of $2.8 billion, with bookings hitting a total of $12 billion.\n",
        "\n",
        "About Uber\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kj5FNstK4n46"
      },
      "source": [
        "Parseamos el texto de entrada con el nuevo pipeline que montamos en la sección anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hxJ_Uzqu4n48",
        "colab": {}
      },
      "source": [
        "doc = nlp(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BxVuaTri4n4-"
      },
      "source": [
        "Aplicamos las reglas para extraer quién (sujeto) hizo (verbo) qué (objeto) en frases donde se ha mencionado alguno de los términos de la ontología."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-6OgNXVP4n4-",
        "outputId": "94298a61-f40d-45c0-c2e0-9718d7410f01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "source": [
        "if doc._.rule_match and len(doc._.rule_match) > 0:\n",
        "    for match in doc._.rule_match:\n",
        "        span = doc[match.start : match.end]  # matched span\n",
        "        sent = span.sent  # sentence containing matched span\n",
        "        match_ents = [{'start': span.start_char - sent.start_char,\n",
        "                       'end': span.end_char - sent.start_char,\n",
        "                       'label': nlp.vocab.strings[match.label]}]\n",
        "        \n",
        "        displacy.render([{'text': sent.text, 'ents': match_ents}], style='ent', manual=True, jupyter=True)\n",
        "       \n",
        "        start_i = sent[0].i\n",
        "        for info in subject_verb_object(sent, start_i):\n",
        "            print(\"Tema:\\t{}\\nSujeto:\\t{}\\nVerbo:\\t{}\\nObjeto:\\t{}\".format(\n",
        "                span.text,\n",
        "                info[0].text,\n",
        "                info[1].text,\n",
        "                info[2].text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Uber pays $148 mn over \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    data breach\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CstSec</span>\n",
              "</mark>\n",
              " in latest image-boosting move. \n",
              "</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Tema:\tdata breach\n",
            "Sujeto:\tUber\n",
            "Verbo:\tpays\n",
            "Objeto:\t148\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">SAN FRANCISCO - Uber agreed Wednesday to pay a $148 million penalty over a massive 2016 \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    data breach\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CstSec</span>\n",
              "</mark>\n",
              " which the company concealed for a year, in the latest effort by the global ridesharing giant to improve its image and move past its missteps from its early years.\n",
              "</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Tema:\tdata breach\n",
            "Sujeto:\tUber\n",
            "Verbo:\tagreed\n",
            "Objeto:\tto pay a $148 million penalty over a\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The payment, described as the largest in a \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    data breach\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CstSec</span>\n",
              "</mark>\n",
              " settlement, is part of Uber's efforts to burnish its reputation after a series of scandals over alleged misconduct and unethical practices.\n",
              "\n",
              "</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Tema:\tdata breach\n",
            "Sujeto:\tpayment\n",
            "Verbo:\tis\n",
            "Objeto:\tpart of Uber\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">In February, Uber agreed to pay $245 million to Alphabet's self-driving car unit Waymo to settle a lawsuit over allegedly stolen \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    trade secrets\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">IntelPty</span>\n",
              "</mark>\n",
              ".\n",
              "\n",
              "</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Tema:\ttrade secrets\n",
            "Sujeto:\tUber\n",
            "Verbo:\tagreed\n",
            "Objeto:\tto pay\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5vO4mL0h4n5A"
      },
      "source": [
        "__The End!__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvfuu3UP4mNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}